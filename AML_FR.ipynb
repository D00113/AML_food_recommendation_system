{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8193a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd, numpy as np, re, math, random, warnings\n",
    "from collections import defaultdict, Counter\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED); random.seed(RANDOM_SEED)\n",
    "\n",
    "# Document Path\n",
    "PATH_GROC = \"Groceries_dataset.csv\"\n",
    "PATH_HSR  = \"au_hsr_cleaned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9affece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load & Basic Clean\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "PATH_GROC = \"Groceries_dataset.csv\"\n",
    "PATH_HSR  = \"au_hsr_cleaned.csv\"\n",
    "\n",
    "# Groceries\n",
    "g = pd.read_csv(PATH_GROC)\n",
    "g.columns = [c.strip() for c in g.columns]\n",
    "\n",
    "# Date\n",
    "g[\"Date\"] = pd.to_datetime(g[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "g = g.dropna(subset=[\"Date\"])\n",
    "\n",
    "def norm_text(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s\\-/&]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "g[\"item_raw\"]  = g[\"itemDescription\"]\n",
    "g[\"item_norm\"] = g[\"itemDescription\"].map(norm_text)\n",
    "\n",
    "# HSR\n",
    "h = pd.read_csv(PATH_HSR)\n",
    "h.columns = [c.strip() for c in h.columns]\n",
    "\n",
    "h[\"hsr_value\"] = pd.to_numeric(h[\"hsr_value\"], errors=\"coerce\")\n",
    "h = h.dropna(subset=[\"hsr_value\"])\n",
    "\n",
    "for c in [\"brands\", \"product_name\", \"category_primary\", \"categories\"]:\n",
    "    if c in h.columns:\n",
    "        h[c + \"_norm\"] = h[c].map(norm_text)\n",
    "\n",
    "def build_hsr_key(row):\n",
    "    parts = []\n",
    "    for c in [\"brands_norm\", \"product_name_norm\", \"category_primary_norm\", \"categories_norm\"]:\n",
    "        if c in row and isinstance(row[c], str) and row[c]:\n",
    "            parts.append(row[c])\n",
    "    return \" \".join(parts).strip()\n",
    "\n",
    "h[\"hsr_key\"] = h.apply(build_hsr_key, axis=1)\n",
    "h = h[h[\"hsr_key\"].str.len() > 0].copy()\n",
    "h.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"[Groceries] rows:\", len(g))\n",
    "print(\"[HSR] rows:\", len(h))\n",
    "print(h[[\"hsr_value\", \"hsr_key\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f46447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HSR and Non-food Classification\n",
    "\n",
    "import os, re, math, gc, random, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import precision_recall_curve, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "PATH_GROC = \"Groceries_dataset.csv\"\n",
    "PATH_HSR  = \"au_hsr_cleaned.csv\"\n",
    "\n",
    "TOPK_SBERT   = 5            \n",
    "SBERT_THR    = 0.45        \n",
    "TFIDF_MIN_DF = 3          \n",
    "SVD_DIM      = 128          \n",
    "RECALL_TGT   = 0.97         \n",
    "\n",
    "SAVE_WITH_ML = \"groceries_with_food_ml.csv\"  \n",
    "SAVE_FINAL   = \"groceries_with_hsr_ml.csv\"    \n",
    "\n",
    "\n",
    "def norm_text(s: str) -> str:\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s\\-\\&/]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def first_like(cols, kw):\n",
    "    cand = [c for c in cols if kw in c.lower()]\n",
    "    return cand[0] if cand else None\n",
    "\n",
    "def build_hsr_key(row: pd.Series, all_cols) -> str:\n",
    "    parts = []\n",
    "    for key in [\"brand\", \"product\", \"subcategory\", \"category\", \"name\", \"description\", \"title\"]:\n",
    "        for c in all_cols:\n",
    "            if key in c and c.endswith(\"_norm\"):\n",
    "                val = row.get(c, \"\")\n",
    "                if isinstance(val, str) and val:\n",
    "                    parts.append(val)\n",
    "    return \" \".join(parts).strip()\n",
    "\n",
    "\n",
    "\n",
    "# Read and clean\n",
    "g = pd.read_csv(PATH_GROC)\n",
    "h = pd.read_csv(PATH_HSR)\n",
    "\n",
    "g[\"item_norm\"] = g[\"itemDescription\"].map(norm_text)\n",
    "\n",
    "hsr_col = None\n",
    "for c in h.columns:\n",
    "    if \"hsr\" in c.lower():\n",
    "        try:\n",
    "            h[c] = pd.to_numeric(h[c], errors=\"coerce\")\n",
    "            if h[c].notna().any():\n",
    "                hsr_col = c\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "assert hsr_col is not None, \"HSR not found\"\n",
    "\n",
    "for c in h.select_dtypes(\"object\").columns:\n",
    "    h[c + \"_norm\"] = h[c].map(norm_text)\n",
    "\n",
    "h[\"hsr_key\"] = h.apply(lambda r: build_hsr_key(r, h.columns), axis=1)\n",
    "h = h[h[\"hsr_key\"].str.len() > 0].copy()\n",
    "h = h.dropna(subset=[hsr_col])\n",
    "h.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# SBERT\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[info] using device: {device}\")\n",
    "\n",
    "sbert = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "emb_g = sbert.encode(\n",
    "    g[\"item_norm\"].tolist(),\n",
    "    batch_size=256,\n",
    "    normalize_embeddings=True,\n",
    "    convert_to_tensor=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "emb_h = sbert.encode(\n",
    "    h[\"hsr_key\"].tolist(),\n",
    "    batch_size=256,\n",
    "    normalize_embeddings=True,\n",
    "    convert_to_tensor=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "idxs, sims = [], []\n",
    "with torch.inference_mode():\n",
    "    for i in range(0, emb_g.size(0), 2048):\n",
    "        q = emb_g[i:i+2048]\n",
    "        cos = util.cos_sim(q, emb_h)     \n",
    "        svals, sidx = torch.topk(cos, k=TOPK_SBERT, dim=1)\n",
    "        idxs.append(sidx.cpu().numpy())\n",
    "        sims.append(svals.cpu().numpy())\n",
    "\n",
    "idxs = np.vstack(idxs)\n",
    "sims = np.vstack(sims)\n",
    "\n",
    "HSR = h[hsr_col].to_numpy()\n",
    "\n",
    "hsr_sbert, src_sbert = [], []\n",
    "for r in range(idxs.shape[0]):\n",
    "    top_idx, top_sim = idxs[r], sims[r]\n",
    "    mask = top_sim >= SBERT_THR\n",
    "    if mask.sum() == 0:\n",
    "        hsr_sbert.append(np.nan)\n",
    "        src_sbert.append(\"sbert<thr\")\n",
    "    else:\n",
    "        w, v = top_sim[mask], HSR[top_idx[mask]]\n",
    "        hsr_sbert.append(float(np.average(v, weights=w)))\n",
    "        src_sbert.append(\"sbert\")\n",
    "\n",
    "g[\"hsr_sbert\"] = hsr_sbert\n",
    "g[\"assign_src\"] = src_sbert   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Ridge\n",
    "tfidf_reg = make_pipeline(\n",
    "    TfidfVectorizer(min_df=TFIDF_MIN_DF, ngram_range=(1, 2)),\n",
    "    Ridge(alpha=1.0, random_state=RANDOM_SEED)\n",
    ")\n",
    "tfidf_reg.fit(h[\"hsr_key\"], h[hsr_col])\n",
    "\n",
    "mask = g[\"hsr_sbert\"].isna()\n",
    "if mask.any():\n",
    "    g.loc[mask, \"hsr_tfidf\"] = tfidf_reg.predict(g.loc[mask, \"item_norm\"])\n",
    "    g.loc[mask, \"assigned_hsr_ml\"] = g.loc[mask, \"hsr_tfidf\"]\n",
    "    g.loc[mask, \"assign_src\"] = \"tfidf\"\n",
    "g.loc[~mask, \"assigned_hsr_ml\"] = g.loc[~mask, \"hsr_sbert\"]\n",
    "g[\"assigned_hsr_ml\"] = g[\"assigned_hsr_ml\"].clip(0, 5)\n",
    "\n",
    "print(\"Coverage:\", float(g[\"assigned_hsr_ml\"].notna().mean()))\n",
    "print(g[\"assign_src\"].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "g[\"assigned_hsr_ml_disc\"] = (np.round(g[\"assigned_hsr_ml\"] * 2) / 2).clip(0, 5)\n",
    "\n",
    "\n",
    "item_hsr_cont = dict(zip(g[\"item_norm\"], g[\"assigned_hsr_ml\"]))\n",
    "\n",
    "item_hsr_disp = dict(zip(g[\"item_norm\"], g[\"assigned_hsr_ml_disc\"]))\n",
    "\n",
    "\n",
    "g[\"hsr_top1_idx\"] = idxs[:, 0]\n",
    "g[\"hsr_top1_sim\"] = sims[:, 0]\n",
    "cat_col  = first_like(h.columns, \"category\")\n",
    "sub_col  = first_like(h.columns, \"subcategory\")\n",
    "if cat_col:\n",
    "    g[\"matched_category\"] = h[cat_col].iloc[g[\"hsr_top1_idx\"]].values\n",
    "if sub_col:\n",
    "    g[\"matched_subcategory\"] = h[sub_col].iloc[g[\"hsr_top1_idx\"]].values\n",
    "\n",
    "tfidf_vec = tfidf_reg.named_steps[\"tfidfvectorizer\"]\n",
    "\n",
    "emb_sbert_all = emb_g.detach().cpu().numpy()\n",
    "emb_sbert_all = normalize(emb_sbert_all)\n",
    "\n",
    "tfidf_X   = tfidf_vec.transform(g[\"item_norm\"])\n",
    "svd_k     = max(32, min(SVD_DIM, tfidf_X.shape[1]-1)) if tfidf_X.shape[1] > 1 else 1\n",
    "svd       = TruncatedSVD(n_components=svd_k, random_state=RANDOM_SEED)\n",
    "emb_tfidf = svd.fit_transform(tfidf_X)\n",
    "emb_tfidf = normalize(emb_tfidf)\n",
    "\n",
    "X_all = np.hstack([emb_sbert_all, emb_tfidf]).astype(np.float32)\n",
    "\n",
    "Xg_n = normalize(tfidf_vec.transform(g[\"item_norm\"]))\n",
    "Xh_n = normalize(tfidf_vec.transform(h[\"hsr_key\"]))\n",
    "S = Xg_n @ Xh_n.T\n",
    "tfidf_max_sim = S.max(axis=1).toarray().ravel() if hasattr(S, \"toarray\") else np.asarray(S).max(axis=1).ravel()\n",
    "g[\"tfidf_max_sim\"] = tfidf_max_sim\n",
    "\n",
    "sbert_mask = (g[\"assign_src\"] == \"sbert\")\n",
    "tfidf_mask = (g[\"assign_src\"] == \"tfidf\")\n",
    "\n",
    "def q(col: pd.Series, mask: pd.Series, p: float, default: float) -> float:\n",
    "    vals = col[mask]\n",
    "    vals = vals[np.isfinite(vals)]\n",
    "    if vals.size == 0:\n",
    "        return default\n",
    "    return float(np.nanquantile(vals, p))\n",
    "\n",
    "sbert_hi = q(g[\"hsr_top1_sim\"], sbert_mask, 0.75, 0.60)\n",
    "sbert_lo = q(g[\"hsr_top1_sim\"], sbert_mask, 0.10, 0.30)\n",
    "tfidf_hi = q(g[\"tfidf_max_sim\"], tfidf_mask, 0.75, 0.20)\n",
    "tfidf_lo = q(g[\"tfidf_max_sim\"], tfidf_mask, 0.10, 0.08)\n",
    "\n",
    "pos_seed = (sbert_mask & (g[\"hsr_top1_sim\"] >= sbert_hi)) | (tfidf_mask & (g[\"tfidf_max_sim\"] >= tfidf_hi))\n",
    "neg_seed = (sbert_mask & (g[\"hsr_top1_sim\"] <= sbert_lo)) | (tfidf_mask & (g[\"tfidf_max_sim\"] <= tfidf_lo))\n",
    "\n",
    "seed_mask = (pos_seed | neg_seed)\n",
    "X_seed    = X_all[seed_mask.values]\n",
    "y_seed    = pos_seed[seed_mask].astype(int).values\n",
    "\n",
    "# 若仍只有一个类别，放宽一次阈值\n",
    "if y_seed.min() == y_seed.max():\n",
    "    sbert_lo2 = q(g[\"hsr_top1_sim\"], sbert_mask, 0.20, 0.35)\n",
    "    tfidf_lo2 = q(g[\"tfidf_max_sim\"], tfidf_mask, 0.20, 0.10)\n",
    "    neg_seed  = (sbert_mask & (g[\"hsr_top1_sim\"] <= sbert_lo2)) | (tfidf_mask & (g[\"tfidf_max_sim\"] <= tfidf_lo2))\n",
    "    seed_mask = (pos_seed | neg_seed)\n",
    "    X_seed    = X_all[seed_mask.values]\n",
    "    y_seed    = pos_seed[seed_mask].astype(int).values\n",
    "\n",
    "if y_seed.min() == y_seed.max():\n",
    "    raise ValueError(\n",
    "        \"Seed generation still produced one class. \"\n",
    "    )\n",
    "\n",
    "clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "clf.fit(X_seed, y_seed)\n",
    "\n",
    "print(\"\\n[Food-ML] seed classification report\")\n",
    "print(classification_report(y_seed, clf.predict(X_seed), digits=4))\n",
    "\n",
    "proba_seed = clf.predict_proba(X_seed)[:, 1]\n",
    "prec, rec, thr = precision_recall_curve(y_seed, proba_seed)\n",
    "\n",
    "prec_aligned = prec[:-1]\n",
    "rec_aligned  = rec[:-1]\n",
    "thr_aligned  = thr\n",
    "\n",
    "mask = (rec_aligned >= RECALL_TGT)\n",
    "if mask.any():\n",
    "    best_idx = int(np.argmax(prec_aligned[mask]))\n",
    "    th_best  = float(thr_aligned[mask][best_idx])\n",
    "else:\n",
    "    th_best = float(np.quantile(proba_seed, 0.5))\n",
    "\n",
    "proba_all = clf.predict_proba(X_all)[:, 1]\n",
    "g[\"is_food_score\"] = proba_all\n",
    "g[\"is_food_ml\"]    = (proba_all >= th_best).astype(int)\n",
    "\n",
    "print(f\"[Food-ML] seeds: +{int(y_seed.sum())}/{len(y_seed)}  \"\n",
    "      f\"thr={th_best:.3f}  \"\n",
    "      f\"pred_food_ratio={g['is_food_ml'].mean():.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "g.to_csv(SAVE_WITH_ML, index=False)\n",
    "\n",
    "if \"Date\" in g.columns:\n",
    "    g[\"Date\"] = pd.to_datetime(g[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "g.to_csv(SAVE_FINAL, index=False)\n",
    "print(f\"Saved -> {SAVE_FINAL} (含 is_food_ml & assigned_hsr_ml)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6796780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_food_score_hist(df: pd.DataFrame, thr: float, bins: int = 50):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(df[\"is_food_score\"], bins=bins, alpha=0.75)\n",
    "    plt.axvline(thr, ls=\"--\", label=f\"thr={thr:.3f}\")\n",
    "    plt.legend(); plt.title(\"Food probability distribution\")\n",
    "    plt.xlabel(\"is_food_score\"); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def show_top_examples(df: pd.DataFrame, label: str = \"food\", topn: int = 12):\n",
    "    if label == \"food\":\n",
    "        sub = df.sort_values(\"is_food_score\", ascending=False).head(topn)\n",
    "    else:\n",
    "        sub = df.sort_values(\"is_food_score\", ascending=True).head(topn)\n",
    "    cols = [\"itemDescription\",\"item_norm\",\"is_food_score\",\"assign_src\",\"assigned_hsr_ml\"]\n",
    "    cols = [c for c in cols if c in sub.columns]\n",
    "    print(f\"\\n[Top {topn}] {label} examples:\")\n",
    "    print(sub[cols].to_string(index=False)[:2000])  \n",
    "\n",
    "def seed_confusion(df: pd.DataFrame):\n",
    "    seed_food = (df[\"assign_src\"].isin([\"sbert\",\"tfidf\"]))\n",
    "    seed_non  = (~df[\"assign_src\"].isin([\"sbert\",\"tfidf\"]))\n",
    "    both = df[(seed_food | seed_non)].copy()\n",
    "    if both.empty:\n",
    "        print(\"No seeds found.\"); return\n",
    "    y_true = seed_food.loc[both.index].astype(int)\n",
    "    y_pred = both[\"is_food_ml\"].astype(int)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(\"\\n[Seed-based confusion]\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "plot_food_score_hist(g, th_best, bins=60)\n",
    "show_top_examples(g, \"food\", topn=10)\n",
    "show_top_examples(g, \"non\",  topn=10)\n",
    "seed_confusion(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f9bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "use = pd.read_csv(\"groceries_with_hsr_ml.csv\", low_memory=False)\n",
    "\n",
    "use = use[(use.get(\"is_food\", True) == True) & (use[\"assigned_hsr_ml\"].notna())].copy()\n",
    "\n",
    "if \"Date\" not in use.columns:\n",
    "    raise ValueError(\"Date not found\")\n",
    "\n",
    "use[\"Date\"] = pd.to_datetime(use[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "use = use.dropna(subset=[\"Date\"])\n",
    "\n",
    "use = use.sort_values([\"Member_number\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "item_hsr = (\n",
    "    use.drop_duplicates(\"item_norm\")\n",
    "       .set_index(\"item_norm\")[\"assigned_hsr_ml\"]\n",
    "       .to_dict()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81eb0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Split + Item2Vec (train-only)\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "use = use.sort_values([\"Member_number\",\"Date\"])\n",
    "t1, t2 = use[\"Date\"].quantile(0.6), use[\"Date\"].quantile(0.8)\n",
    "train_df = use[use[\"Date\"] <= t1].copy()\n",
    "val_df   = use[(use[\"Date\"] > t1) & (use[\"Date\"] <= t2)].copy()\n",
    "test_df  = use[use[\"Date\"] > t2].copy()\n",
    "\n",
    "# Item2Vec\n",
    "baskets = (train_df.groupby([\"Member_number\",\"Date\"])[\"item_norm\"].apply(list).tolist())\n",
    "freq = Counter([x for s in baskets for x in s])\n",
    "sentences = [[w for w in s if freq[w] >= 5] for s in baskets]\n",
    "sentences = [s for s in sentences if len(s) >= 2]\n",
    "\n",
    "w2v = Word2Vec(sentences=sentences, vector_size=64, window=5, sg=1,\n",
    "               negative=10, min_count=5, seed=RANDOM_SEED, workers=4, epochs=10)\n",
    "\n",
    "def item_vec(it):\n",
    "    return w2v.wv[it] if it in w2v.wv else np.zeros(64, dtype=np.float32)\n",
    "\n",
    "user_hist_train = train_df.groupby(\"Member_number\")[\"item_norm\"].apply(list).to_dict()\n",
    "def user_vec(u):\n",
    "    items = user_hist_train.get(u, [])\n",
    "    vecs = [item_vec(i) for i in items if i in w2v.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(64, dtype=np.float32)\n",
    "\n",
    "user_vecs = {u: user_vec(u) for u in use[\"Member_number\"].unique()}\n",
    "\n",
    "\n",
    "def build_true_pos_map(df):\n",
    "    return (df.groupby(\"Member_number\")[\"item_norm\"]\n",
    "              .apply(lambda s: set(s.tolist()))\n",
    "              .to_dict())\n",
    "\n",
    "true_pos_map_val  = build_true_pos_map(val_df)\n",
    "true_pos_map_test = build_true_pos_map(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb07f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute reduced item vectors (PCA-16), \n",
    "# popularity percentile, \n",
    "# category-level HSR percentile, \n",
    "# and user–category recency\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "all_items = sorted(use[\"item_norm\"].unique())\n",
    "emb_dim = w2v.vector_size\n",
    "E = np.vstack([w2v.wv[i] if i in w2v.wv else np.zeros(emb_dim) for i in all_items])\n",
    "pca = PCA(n_components=16, random_state=RANDOM_SEED)\n",
    "E16 = pca.fit_transform(E)\n",
    "item_vec16 = {i: E16[idx].astype(np.float32) for idx, i in enumerate(all_items)}\n",
    "\n",
    "pop_counts = train_df[\"item_norm\"].value_counts()\n",
    "ranks = pop_counts.rank(method=\"average\", pct=True)\n",
    "pop_pct = ranks.to_dict() \n",
    "\n",
    "\n",
    "cat_col_name = \"matched_category\" if \"matched_category\" in use.columns else None\n",
    "item_cat = (use.drop_duplicates(\"item_norm\")\n",
    "              .set_index(\"item_norm\")[cat_col_name]\n",
    "              .fillna(\"misc\").astype(str).to_dict()) if cat_col_name else {i:\"misc\" for i in all_items}\n",
    "\n",
    "tmp = (train_df.groupby([\"item_norm\"])[\"assigned_hsr_ml\"].mean().rename(\"hsr_mean\").reset_index())\n",
    "tmp[\"cat\"] = tmp[\"item_norm\"].map(item_cat)\n",
    "cat_hsr_pct = {}\n",
    "for c, dfc in tmp.groupby(\"cat\"):\n",
    "    dfc = dfc.sort_values(\"hsr_mean\")\n",
    "    dfc[\"pct\"] = np.linspace(0, 1, len(dfc))\n",
    "    for r in dfc.itertuples():\n",
    "        cat_hsr_pct[r.item_norm] = float(r.pct)\n",
    "\n",
    "\n",
    "\n",
    "last_date_uc = {}\n",
    "for u, g_u in train_df.groupby(\"Member_number\"):\n",
    "    d = {}\n",
    "    for c, g_c in g_u.assign(cat=g_u[\"item_norm\"].map(item_cat)).groupby(\"cat\"):\n",
    "        d[c] = g_c[\"Date\"].max()\n",
    "    last_date_uc[u] = d\n",
    "\n",
    "def recency_score(u, i):\n",
    "    c = item_cat.get(i, \"misc\")\n",
    "    t_last = last_date_uc.get(u, {}).get(c, None)\n",
    "    if t_last is None:\n",
    "        return 0.0\n",
    "    delta = (train_df[\"Date\"].max() - t_last).days\n",
    "    return float(np.exp(-delta / 30.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2924e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sampling + feature augmentation + in-group label check\n",
    "import numpy as np, lightgbm as lgb, random, math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    na, nb = np.linalg.norm(a), np.linalg.norm(b)\n",
    "    return float(np.dot(a, b) / (na * nb)) if na > 0 and nb > 0 else 0.0\n",
    "\n",
    "cats = sorted(pd.Series(list(item_cat.values())).unique())\n",
    "cat2id = {c: i for i, c in enumerate(cats)}\n",
    "cat_items = defaultdict(list)\n",
    "for i, c in item_cat.items():\n",
    "    cat_items[c].append(i)\n",
    "\n",
    "def item_vec16_or_zero(i):\n",
    "    return item_vec16.get(i, np.zeros(16, dtype=np.float32))\n",
    "\n",
    "def build_features(u, i):\n",
    "    v_i = item_vec(i)      \n",
    "    uv  = user_vecs.get(u, np.zeros_like(v_i))\n",
    "    pref = cos_sim(uv, v_i)\n",
    "\n",
    "    hsr = float(train_df.loc[train_df[\"item_norm\"]==i, \"assigned_hsr_ml\"].mean())/5.0 \\\n",
    "          if (train_df[\"item_norm\"]==i).any() else 0.0\n",
    "\n",
    "    popn = float(pop_pct.get(i, 0.0))\n",
    "    catpct = float(cat_hsr_pct.get(i, 0.5))\n",
    "    rec = recency_score(u, i)\n",
    "\n",
    "    vec16 = item_vec16_or_zero(i)\n",
    "    onehot = np.zeros(len(cats), dtype=np.float32)\n",
    "    c = item_cat.get(i, \"misc\")\n",
    "    if c in cat2id:\n",
    "        onehot[cat2id[c]] = 1.0\n",
    "\n",
    "    base = np.array([pref, hsr, popn, catpct, rec], dtype=np.float32)\n",
    "    return np.concatenate([base, vec16, onehot]).astype(np.float32)\n",
    "\n",
    "# Negative Sampling\n",
    "def sample_negatives(u, k_in=80, k_cross=20):\n",
    "    seen = set(user_hist_train.get(u, []))\n",
    "    seen_cats = {item_cat.get(x, \"misc\") for x in seen} or {\"misc\"}\n",
    "\n",
    "    cands = []\n",
    "    for c in seen_cats:\n",
    "        pool = [it for it in cat_items.get(c, []) if it not in seen]\n",
    "        random.shuffle(pool)\n",
    "        cands += pool[:k_in // max(1, len(seen_cats))]\n",
    "    top_cats = sorted(cats, key=lambda c: len(cat_items.get(c, [])), reverse=True)[:3]\n",
    "    for c in top_cats:\n",
    "        if c in seen_cats: \n",
    "            continue\n",
    "        pool = [it for it in cat_items.get(c, []) if it not in seen]\n",
    "        random.shuffle(pool)\n",
    "        cands += pool[:k_cross // max(1, (3 - len(seen_cats)))]\n",
    "\n",
    "    return list(dict.fromkeys(cands))\n",
    "\n",
    "\n",
    "all_users = sorted(user_hist_train.keys())\n",
    "tr_u, va_u = train_test_split(all_users, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
    "\n",
    "def build_user_samples(u):\n",
    "    pos = list(set(user_hist_train.get(u, [])))\n",
    "    if not pos:\n",
    "        return None\n",
    "    neg = [i for i in sample_negatives(u) if i not in pos]\n",
    "    if not neg:\n",
    "        return None\n",
    "    cand = list(dict.fromkeys(pos + neg))\n",
    "    y = [1 if i in pos else 0 for i in cand]\n",
    "    if (sum(y) == 0) or (sum(y) == len(y)):\n",
    "        return None\n",
    "    X = [build_features(u, i) for i in cand]\n",
    "    return X, y, len(cand)\n",
    "\n",
    "X_tr_list, y_tr_list, group_tr = [], [], []\n",
    "X_va_list, y_va_list, group_va = [], [], []\n",
    "drop_tr = drop_va = 0\n",
    "\n",
    "\n",
    "for u in tr_u:\n",
    "    out = build_user_samples(u)\n",
    "    if out is None:\n",
    "        drop_tr += 1; continue\n",
    "    X_u, y_u, n_u = out\n",
    "    X_tr_list.extend(X_u); y_tr_list.extend(y_u); group_tr.append(n_u)\n",
    "\n",
    "\n",
    "for u in va_u:\n",
    "    out = build_user_samples(u)\n",
    "    if out is None:\n",
    "        drop_va += 1; continue\n",
    "    X_u, y_u, n_u = out\n",
    "    X_va_list.extend(X_u); y_va_list.extend(y_u); group_va.append(n_u)\n",
    "\n",
    "print(f\"[info] valid train groups: {len(group_tr)}, dropped: {drop_tr}\")\n",
    "print(f\"[info] valid valid  groups: {len(group_va)}, dropped: {drop_va}\")\n",
    "\n",
    "X_tr = np.array(X_tr_list, dtype=np.float32)\n",
    "y_tr = np.array(y_tr_list, dtype=np.int32)\n",
    "X_va = np.array(X_va_list, dtype=np.float32)\n",
    "y_va = np.array(y_va_list, dtype=np.int32)\n",
    "\n",
    "train_set = lgb.Dataset(X_tr, label=y_tr, group=group_tr)\n",
    "valid_set = lgb.Dataset(X_va, label=y_va, group=group_va, reference=train_set)\n",
    "\n",
    "pos = (y_tr == 1).sum()\n",
    "neg = (y_tr == 0).sum()\n",
    "scale = float(neg) / max(1, pos)\n",
    "\n",
    "params = dict(\n",
    "    objective=\"binary\",\n",
    "    metric=[\"auc\", \"binary_logloss\"],\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=127,\n",
    "    min_data_in_leaf=50,              \n",
    "    min_sum_hessian_in_leaf=1e-3,\n",
    "    feature_fraction=0.9,\n",
    "    lambda_l2=1.0,\n",
    "    is_unbalance=True,               \n",
    "    max_bin=255,                      \n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ranker = lgb.train(\n",
    "    params,\n",
    "    train_set,\n",
    "    valid_sets=[valid_set],\n",
    "    num_boost_round=1200,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=80), lgb.log_evaluation(period=50)]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "if ranker.current_iteration() <= 1:\n",
    "    print(\"[warn] lambdarank degenerate; fallback to pointwise 'binary'\")\n",
    "    params_pw = dict(objective=\"binary\", metric=[\"auc\", \"binary_logloss\"],\n",
    "                     learning_rate=0.05, num_leaves=127, min_data_in_leaf=20,\n",
    "                     min_sum_hessian_in_leaf=1e-3, feature_fraction=0.9,\n",
    "                     lambda_l2=1.0, random_state=RANDOM_SEED)\n",
    "    ranker = lgb.train(\n",
    "        params_pw, train_set, valid_sets=[valid_set],\n",
    "        num_boost_round=1200,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=80), lgb.log_evaluation(period=50)]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909bcaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Candidates + Ranking Metrics\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "user_val_pos  = val_df.groupby(\"Member_number\")[\"item_norm\"].apply(set).to_dict()\n",
    "user_test_pos = test_df.groupby(\"Member_number\")[\"item_norm\"].apply(set).to_dict()\n",
    "\n",
    "def candidate_with_positives(u, phase=\"val\", k=50):\n",
    "    pos = list((user_val_pos if phase==\"val\" else user_test_pos).get(u, set()))\n",
    "    neg = sample_negatives(u, k_in=k, k_cross=10)  \n",
    "\n",
    "    return list(dict.fromkeys(pos + neg)), pos\n",
    "\n",
    "\n",
    "def build_eval_packs(df_phase, phase=\"val\", k=50):\n",
    "    packs = []\n",
    "    for u in df_phase[\"Member_number\"].unique():\n",
    "        cand, pos = candidate_with_positives(u, phase, k)\n",
    "        if not cand: \n",
    "            continue\n",
    "        feats = np.vstack([build_features(u, i) for i in cand])\n",
    "        scores = ranker.predict(feats, num_iteration=ranker.best_iteration)\n",
    "        ytrue = np.array([1 if i in pos else 0 for i in cand])\n",
    "        packs.append((u, cand, ytrue, scores))\n",
    "    return packs\n",
    "\n",
    "val_packs  = build_eval_packs(val_df,  \"val\",  50)\n",
    "test_packs = build_eval_packs(test_df, \"test\", 50)\n",
    "\n",
    "\n",
    "\n",
    "def blend_scores(base_scores: np.ndarray, item_hsr_values: np.ndarray, alpha: float) -> np.ndarray:\n",
    "    z = np.asarray(base_scores, dtype=float)\n",
    "    h01 = np.asarray(item_hsr_values, dtype=float) / 5.0  \n",
    "    return (1.0 - alpha) * z + alpha * h01\n",
    "\n",
    "\n",
    "def round_hsr_for_display(h: np.ndarray) -> np.ndarray:\n",
    "    return np.round(h * 2) / 2.0\n",
    "\n",
    "\n",
    "def ranking_panel(packs, k=10):\n",
    "    P, N = [], []\n",
    "    for _, items, ytrue, scores in packs:\n",
    "        if len(items) < 2: \n",
    "            continue\n",
    "        idx = np.argsort(-scores)[:k]\n",
    "        P.append(ytrue[idx].mean())\n",
    "        N.append(ndcg_score([ytrue], [scores], k=k))\n",
    "    return dict(\n",
    "        Precision_k = float(np.mean(P)) if P else 0.0,\n",
    "        NDCG_k      = float(np.mean(N)) if N else 0.0\n",
    "    )\n",
    "\n",
    "print(\"RANKING-VAL@10 :\", ranking_panel(val_packs, 10))\n",
    "print(\"RANKING-TEST@10:\", ranking_panel(test_packs, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a628aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def _get_item_category(it: str, use_df: pd.DataFrame) -> str | None:\n",
    "    sel = use_df.loc[use_df[\"item_norm\"] == it, [\"matched_category\", \"refined_category\"]] \\\n",
    "                if \"refined_category\" in use_df.columns \\\n",
    "                else use_df.loc[use_df[\"item_norm\"] == it, [\"matched_category\"]]\n",
    "    if sel.empty:\n",
    "        return None\n",
    "    cat = sel[\"matched_category\"].iloc[0] if \"matched_category\" in sel.columns else None\n",
    "    if (cat is None) or (pd.isna(cat)) or (str(cat).strip() == \"\"):\n",
    "        if \"refined_category\" in sel.columns:\n",
    "            cat = sel[\"refined_category\"].iloc[0]\n",
    "    if (cat is None) or (pd.isna(cat)) or (str(cat).strip() == \"\"):\n",
    "        return None\n",
    "    return str(cat)\n",
    "\n",
    "\n",
    "def _build_true_by_cat_for_user(u, true_pos_map: dict, use_df: pd.DataFrame, item_hsr_dict: dict):\n",
    "    by_cat = defaultdict(list)\n",
    "    true_set = true_pos_map.get(u, set())\n",
    "    for it in true_set:\n",
    "        c = _get_item_category(it, use_df)\n",
    "        h = item_hsr_dict.get(it, np.nan)\n",
    "        if (c is not None) and (h == h):\n",
    "            by_cat[c].append(float(h))\n",
    "    return by_cat, true_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6451370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health: HealthGain@k + LowHSR@k + Coverage）\n",
    "\n",
    "def health_panel(packs, true_pos_map: dict, item_hsr_dict: dict, use_df: pd.DataFrame, k=10):\n",
    "    Gain, Low, covered = [], [], 0\n",
    "    total = len(packs)\n",
    "\n",
    "    all_item_hsr_vals = [v for v in item_hsr_dict.values() if v == v]\n",
    "    global_mean = float(np.mean(all_item_hsr_vals)) if all_item_hsr_vals else np.nan\n",
    "\n",
    "    for u, items, ytrue, scores in packs:\n",
    "\n",
    "        idx = np.argsort(-scores)[:k]\n",
    "        top_items = [items[t] for t in idx]\n",
    "\n",
    "        true_by_cat, true_set = _build_true_by_cat_for_user(u, true_pos_map, use_df, item_hsr_dict)\n",
    "        if len(true_set) == 0:\n",
    "            Low.append(0.0)\n",
    "            continue\n",
    "\n",
    "        gains, lows = [], []\n",
    "        for it in top_items:\n",
    "            rec_hsr = item_hsr_dict.get(it, np.nan)\n",
    "            c = _get_item_category(it, use_df)\n",
    "\n",
    "\n",
    "            base = np.nan\n",
    "            if (c in true_by_cat) and (len(true_by_cat[c]) > 0):\n",
    "                base = np.nanmean(true_by_cat[c])\n",
    "            else:\n",
    "                all_u = [v for vv in true_by_cat.values() for v in vv]\n",
    "                base = np.nanmean(all_u) if len(all_u) > 0 else np.nan\n",
    "            if not (base == base): \n",
    "                base = global_mean\n",
    "\n",
    "            if (rec_hsr == rec_hsr) and (base == base):\n",
    "                gains.append((rec_hsr - base) / 5.0) \n",
    "\n",
    "            lows.append(1 if (rec_hsr == rec_hsr and rec_hsr <= 2.5) else 0)\n",
    "\n",
    "        if gains:\n",
    "            Gain.append(float(np.mean(gains)))\n",
    "            covered += 1\n",
    "        Low.append(float(np.mean(lows)) if lows else 0.0)\n",
    "\n",
    "    return dict(\n",
    "        HealthGain_k = float(np.mean(Gain)) if len(Gain) > 0 else 0.0,\n",
    "        LowHSR_k     = float(np.mean(Low))  if len(Low)  > 0 else 0.0,\n",
    "        HealthEvaluableCoverage = float(100.0 * covered / total) if total > 0 else 0.0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1636a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_scores(base_scores, item_hsr_values, alpha):\n",
    "    import numpy as np\n",
    "    z = np.asarray(base_scores, dtype=np.float32)\n",
    "    s_min, s_max = float(np.min(z)), float(np.max(z))\n",
    "    z = (z - s_min) / (s_max - s_min) if s_max > s_min else np.zeros_like(z)\n",
    "\n",
    "    h = np.asarray(item_hsr_values, dtype=np.float32)\n",
    "    if np.isnan(h).any():\n",
    "        h_mean = float(np.nanmean(h))\n",
    "        h = np.where(np.isnan(h), h_mean if h_mean == h_mean else 0.0, h)\n",
    "    h01 = h / 5.0\n",
    "\n",
    "    return (1.0 - alpha) * z + alpha * h01\n",
    "\n",
    "\n",
    "\n",
    "def alpha_rerank_panels(\n",
    "    packs,\n",
    "    true_pos_map: dict,\n",
    "    item_hsr_dict: dict,            \n",
    "    use_df: pd.DataFrame,\n",
    "    alpha: float = 0.6,\n",
    "    k: int = 10,\n",
    "    item_hsr_display_dict: dict | None = None \n",
    "):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import ndcg_score\n",
    "\n",
    "    P, N = [], []\n",
    "    Gain, Low, covered = [], [], 0\n",
    "    total = len(packs)\n",
    "\n",
    "    all_vals = [v for v in item_hsr_dict.values() if v == v]\n",
    "    global_mean = float(np.mean(all_vals)) if all_vals else np.nan\n",
    "\n",
    "    disp_dict = item_hsr_display_dict if item_hsr_display_dict is not None else item_hsr_dict\n",
    "\n",
    "    for u, items, ytrue, scores in packs:\n",
    "        true_by_cat, true_set = _build_true_by_cat_for_user(u, true_pos_map, use_df, disp_dict)\n",
    "\n",
    "        h_raw = np.array([item_hsr_dict.get(it, np.nan) for it in items], dtype=np.float32)\n",
    "        if np.isnan(h_raw).any():\n",
    "            cm = float(np.nanmean(h_raw))\n",
    "            h_raw = np.where(np.isnan(h_raw), cm if cm == cm else global_mean, h_raw)\n",
    "        h_raw = np.nan_to_num(h_raw, nan=(global_mean if global_mean == global_mean else 0.0))\n",
    "        hybrid = blend_scores(scores, h_raw, alpha)\n",
    "\n",
    "        if len(items) >= 2:\n",
    "            idx = np.argsort(-hybrid)[:k]\n",
    "            P.append(float(np.mean(np.take(ytrue, idx))))\n",
    "            N.append(float(ndcg_score([ytrue], [hybrid], k=k)))\n",
    "\n",
    "        if len(true_set) > 0:\n",
    "            idx = np.argsort(-hybrid)[:k]\n",
    "            top_items = [items[t] for t in idx]\n",
    "            gains, lows = [], []\n",
    "            for it in top_items:\n",
    "                rec = disp_dict.get(it, np.nan)             \n",
    "                c = _get_item_category(it, use_df)\n",
    "\n",
    "                if (c in true_by_cat) and (len(true_by_cat[c]) > 0):\n",
    "                    base = float(np.nanmean(true_by_cat[c])) \n",
    "                else:\n",
    "                    all_u = [v for vv in true_by_cat.values() for v in vv]\n",
    "                    base = float(np.nanmean(all_u)) if len(all_u) > 0 else np.nan\n",
    "\n",
    "                if (rec == rec) and (base == base):\n",
    "                    gains.append((rec - base) / 5.0)\n",
    "                lows.append(1 if (rec == rec and rec <= 2.5) else 0)\n",
    "\n",
    "            if gains:\n",
    "                Gain.append(float(np.mean(gains))); covered += 1\n",
    "            Low.append(float(np.mean(lows)) if lows else 0.0)\n",
    "        else:\n",
    "            Low.append(0.0)\n",
    "\n",
    "    return dict(\n",
    "        RANKING = {\n",
    "            \"Precision_k\": float(np.mean(P)) if P else 0.0,\n",
    "            \"NDCG_k\"     : float(np.mean(N)) if N else 0.0\n",
    "        },\n",
    "        HEALTH  = {\n",
    "            \"HealthGain_k\"           : float(np.mean(Gain)) if Gain else 0.0,\n",
    "            \"LowHSR_k\"               : float(np.mean(Low))  if Low  else 0.0,\n",
    "            \"HealthEvaluableCoverage\": float(100.0 * covered / total) if total > 0 else 0.0\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HEALTH-VAL@10 :\",  health_panel(val_packs,  user_val_pos,  item_hsr, use, k=10))\n",
    "print(\"HEALTH-TEST@10:\",  health_panel(test_packs, user_test_pos, item_hsr, use, k=10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e26434",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in [0.2, 0.4, 0.6, 0.8]:\n",
    "    print(f\"alpha={a}  VAL :\",  alpha_rerank_panels(val_packs,  user_val_pos,  item_hsr, use, alpha=a, k=10))\n",
    "for a in [0.2, 0.4, 0.6, 0.8]:\n",
    "    print(f\"alpha={a}  TEST:\", alpha_rerank_panels(test_packs, user_test_pos, item_hsr, use, alpha=a, k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5194bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# α grid search + Bootstrap CI \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ndcg@k\n",
    "def _ndcg_at_k(y_true, scores, k=10):\n",
    "    if len(y_true) == 0: \n",
    "        return 0.0\n",
    "    idx = np.argsort(-np.asarray(scores))[:k]\n",
    "    gains = (2**np.asarray(y_true)[idx] - 1)\n",
    "    discounts = 1.0 / np.log2(np.arange(2, len(idx) + 2))\n",
    "    dcg = np.sum(gains * discounts)\n",
    "\n",
    "    # ideal dcg\n",
    "    ideal_idx = np.argsort(-np.asarray(y_true))[:k]\n",
    "    ideal_gains = (2**np.asarray(y_true)[ideal_idx] - 1)\n",
    "    ideal_dcg = np.sum(ideal_gains * discounts)\n",
    "    return float(dcg / ideal_dcg) if ideal_dcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def _eval_on_packs(packs, item_hsr, alpha=0.4, k=10):\n",
    "    P, N, Hgain, LowHSR = [], [], [], []\n",
    "\n",
    "    for (_, cand, ytrue, base_scores) in packs:\n",
    "        if len(cand) == 0:\n",
    "            continue\n",
    "\n",
    "        h = np.array([item_hsr.get(it, np.nan) for it in cand], dtype=float)\n",
    "        if np.isnan(h).all():\n",
    "            continue\n",
    "\n",
    "        h_cand_mean = np.nanmean(h)\n",
    "        h = np.where(np.isnan(h), h_cand_mean, h)\n",
    "\n",
    "\n",
    "        h01 = h / 5.0\n",
    "\n",
    "\n",
    "        z = np.asarray(base_scores, dtype=float)\n",
    "        hybrid = (1 - alpha) * z + alpha * h01\n",
    "\n",
    "\n",
    "        idx = np.argsort(-hybrid)[:k]\n",
    "        hits = np.take(ytrue, idx)\n",
    "        P.append(float(np.mean(hits)))\n",
    "        N.append(_ndcg_at_k(ytrue, hybrid, k=k))\n",
    "\n",
    "\n",
    "        h_topk = h[idx]\n",
    "        Hgain.append(float(np.mean(h_topk) - h_cand_mean))\n",
    "        LowHSR.append(float(np.mean(h_topk <= 2.5)))\n",
    "\n",
    "    return dict(\n",
    "        Precision_k = float(np.mean(P)) if P else 0.0,\n",
    "        NDCG_k      = float(np.mean(N)) if N else 0.0,\n",
    "        HealthGain_k= float(np.mean(Hgain)) if Hgain else 0.0,\n",
    "        LowHSR_k    = float(np.mean(LowHSR)) if LowHSR else 0.0,\n",
    "        Users       = len(P)\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56939c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress-enabled Bootstrap + Grid Search\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "\n",
    "def _bootstrap_metrics(\n",
    "    packs,\n",
    "    item_hsr_cont,\n",
    "    alpha,\n",
    "    k: int = 10,\n",
    "    B: int = 200,\n",
    "    rng: np.random.Generator | None = None,\n",
    "    show_progress: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"User-level bootstrap of metrics for a given alpha (fast evaluator).\"\"\"\n",
    "    rng = rng or np.random.default_rng(42)\n",
    "    users = np.arange(len(packs))\n",
    "    prec, hgain = [], []\n",
    "    iterator = tqdm(range(B), leave=False, desc=f\"bootstrap B={B}, α={alpha:.2f}\") if show_progress else range(B)\n",
    "    for _ in iterator:\n",
    "        idx = rng.choice(users, size=len(users), replace=True)\n",
    "        subpacks = [packs[i] for i in idx]\n",
    "        m = _eval_on_packs(subpacks, item_hsr_cont, alpha=alpha, k=k)\n",
    "        prec.append(m[\"Precision_k\"])\n",
    "        hgain.append(m[\"HealthGain_k\"])\n",
    "    return dict(\n",
    "        precision_mean = float(np.mean(prec)),\n",
    "        precision_std  = float(np.std(prec, ddof=1)),\n",
    "        health_mean    = float(np.mean(hgain)),\n",
    "        health_std     = float(np.std(hgain, ddof=1)),\n",
    "    )\n",
    "\n",
    "def grid_search_alpha_with_ci(\n",
    "    val_packs,\n",
    "    test_packs,\n",
    "    item_hsr,                \n",
    "    item_hsr_display_dict=None,\n",
    "    val_true_pos_map=None,\n",
    "    test_true_pos_map=None,\n",
    "    use_df=None,\n",
    "    k: int = 10,\n",
    "    alphas: np.ndarray | None = None,\n",
    "    B: int = 200,\n",
    "    seed: int = 42,\n",
    "    max_drop: float = 0.05,\n",
    "    autosave_every: int = 5,\n",
    "    tag: str = \"run\",\n",
    "    show_progress: bool = True,\n",
    "):\n",
    "    \"\"\"Alpha grid + bootstrap CIs (fast), with optional one-shot heavy re-eval at the chosen α*.\"\"\"\n",
    "    if alphas is None:\n",
    "        alphas = np.linspace(0.0, 1.0, 21)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows_val, rows_test = [], []\n",
    "\n",
    "    print(\"[info] baseline (α=0) on VAL/TEST\")\n",
    "    base_val  = _eval_on_packs(val_packs,  item_hsr, alpha=0.0, k=k)\n",
    "    base_test = _eval_on_packs(test_packs, item_hsr, alpha=0.0, k=k)\n",
    "    print(\"  VAL :\", base_val)\n",
    "    print(\"  TEST:\", base_test)\n",
    "\n",
    "    iterator = tqdm(alphas, desc=\"α grid\", leave=True) if show_progress else alphas\n",
    "    for i, a in enumerate(iterator, 1):\n",
    "        if show_progress:\n",
    "            iterator.set_postfix_str(f\"α={a:.2f}\")\n",
    "\n",
    "        ci_val = _bootstrap_metrics(val_packs,  item_hsr, alpha=a, k=k, B=B, rng=rng, show_progress=show_progress)\n",
    "        ci_tst = _bootstrap_metrics(test_packs, item_hsr, alpha=a, k=k, B=B, rng=rng, show_progress=show_progress)\n",
    "\n",
    "        rows_val.append(dict(alpha=a, **ci_val))\n",
    "        rows_test.append(dict(alpha=a, **ci_tst))\n",
    "\n",
    "        print(f\"→ α={a:.2f} | TEST Precision={ci_tst['precision_mean']:.4f}±{1.96*ci_tst['precision_std']:.4f}, \"\n",
    "              f\"HealthGain={ci_tst['health_mean']:.4f}±{1.96*ci_tst['health_std']:.4f}\")\n",
    "\n",
    "        if (i % autosave_every) == 0:\n",
    "            pd.DataFrame(rows_val).to_csv(f\"results/tmp_val_{tag}.csv\", index=False)\n",
    "            pd.DataFrame(rows_test).to_csv(f\"results/tmp_test_{tag}.csv\", index=False)\n",
    "            print(f\"[auto-save] partial results -> results/tmp_val_{tag}.csv, results/tmp_test_{tag}.csv\")\n",
    "\n",
    "\n",
    "    df_val  = pd.DataFrame(rows_val)\n",
    "    df_test = pd.DataFrame(rows_test)\n",
    "\n",
    "    base_p = base_test[\"Precision_k\"]\n",
    "    mask = (df_test[\"precision_mean\"] >= (1 - max_drop) * base_p) & (df_test[\"health_mean\"] > 0)\n",
    "    best_row = (df_test[mask].sort_values(\"health_mean\", ascending=False).iloc[0]\n",
    "                if mask.any() else df_test.sort_values(\"precision_mean\", ascending=False).iloc[0])\n",
    "\n",
    "    best = dict(\n",
    "        best_alpha = float(best_row[\"alpha\"]),\n",
    "        precision  = float(best_row[\"precision_mean\"]),\n",
    "        healthgain = float(best_row[\"health_mean\"]),\n",
    "    )\n",
    "\n",
    "\n",
    "    df_val.to_csv (f\"results/df_val_{tag}.csv\",  index=False)\n",
    "    df_test.to_csv(f\"results/df_test_{tag}.csv\", index=False)\n",
    "    with open(f\"results/best_alpha_{tag}.json\", \"w\") as f:\n",
    "        json.dump(best, f, indent=2)\n",
    "\n",
    "    # Plot and save TEST trade-off with 95% CI\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    p_low  = df_test[\"precision_mean\"] - 1.96 * df_test[\"precision_std\"]\n",
    "    p_high = df_test[\"precision_mean\"] + 1.96 * df_test[\"precision_std\"]\n",
    "    plt.fill_between(df_test[\"alpha\"], p_low, p_high, alpha=0.2, label=\"Precision 95% CI\")\n",
    "    p_line, = plt.plot(df_test[\"alpha\"], df_test[\"precision_mean\"], \"-o\", label=f\"Precision@{k}\")\n",
    "    plt.xlabel(\"α (health weight)\")\n",
    "    plt.ylabel(f\"Precision@{k}\", color=p_line.get_color())\n",
    "\n",
    "    # Right axis: HealthGain\n",
    "    ax2 = plt.gca().twinx()\n",
    "    h_low  = df_test[\"health_mean\"] - 1.96 * df_test[\"health_std\"]\n",
    "    h_high = df_test[\"health_mean\"] + 1.96 * df_test[\"health_std\"]\n",
    "    ax2.fill_between(df_test[\"alpha\"], h_low, h_high, alpha=0.2, color=\"C1\", label=\"HealthGain 95% CI\")\n",
    "    h_line, = ax2.plot(df_test[\"alpha\"], df_test[\"health_mean\"], \"-s\", color=\"C1\", label=f\"HealthGain@{k}\")\n",
    "    ax2.set_ylabel(f\"HealthGain@{k}\", color=\"C1\")\n",
    "\n",
    "    # Mark the chosen alpha\n",
    "    plt.axvline(best[\"best_alpha\"], ls=\"--\", color=\"k\", lw=1)\n",
    "    plt.title(f\"Health–Accuracy Trade-off (Bootstrap CI, K={k}) - TEST\")\n",
    "    lines = [p_line, h_line]; labels = [l.get_label() for l in lines]\n",
    "    plt.legend(lines, labels, loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/tradeoff_{tag}.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n[auto] Optimal α on TEST = {best['best_alpha']:.2f} \"\n",
    "          f\"(Precision={best['precision']:.4f}, HealthGain={best['healthgain']:.4f})\")\n",
    "    print(f\"[saved] results/df_val_{tag}.csv, results/df_test_{tag}.csv, results/best_alpha_{tag}.json, results/tradeoff_{tag}.png\")\n",
    "\n",
    "    # Optional heavy re-eval at α* (once, for reporting metrics)\n",
    "    if (item_hsr_display_dict is not None) and (val_true_pos_map is not None) and (use_df is not None):\n",
    "        try:\n",
    "            a_star = best[\"best_alpha\"]\n",
    "            print(f\"\\n[final re-eval @ α*={a_star:.2f}] using heavy metrics...\")\n",
    "            res_val  = alpha_rerank_panels(val_packs,  val_true_pos_map,  item_hsr, use_df,\n",
    "                                           alpha=a_star, k=k, item_hsr_display_dict=item_hsr_display_dict)\n",
    "            res_test = alpha_rerank_panels(test_packs, test_true_pos_map, item_hsr, use_df,\n",
    "                                           alpha=a_star, k=k, item_hsr_display_dict=item_hsr_display_dict)\n",
    "            print(\"[final @ α*] VAL :\",  res_val)\n",
    "            print(\"[final @ α*] TEST:\",  res_test)\n",
    "        except NameError:\n",
    "            print(\"[warn] alpha_rerank_panels is not defined; skip heavy re-eval.\")\n",
    "\n",
    "    return df_val, df_test, best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val, df_test, best = grid_search_alpha_with_ci(\n",
    "    val_packs=val_packs,\n",
    "    test_packs=test_packs,\n",
    "    item_hsr=item_hsr_cont,                 \n",
    "    item_hsr_display_dict=item_hsr_disp,    \n",
    "    val_true_pos_map=true_pos_map_val,      \n",
    "    test_true_pos_map=true_pos_map_test,    \n",
    "    use_df=use,                             \n",
    "    k=10,\n",
    "    alphas=np.linspace(0,1,21),\n",
    "    B=200,\n",
    "    seed=42,\n",
    "    max_drop=0.05,\n",
    "    autosave_every=5,\n",
    "    tag=\"full_run\",\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "heavy_test = alpha_rerank_panels(\n",
    "    test_packs, true_pos_map_test, item_hsr_cont, use,\n",
    "    alpha=float(best[\"best_alpha\"]), k=10, item_hsr_display_dict=item_hsr_disp\n",
    ")\n",
    "pd.Series({**heavy_test[\"RANKING\"], **heavy_test[\"HEALTH\"]}).to_csv(\n",
    "    f\"results/best_alpha_old_main_with_CI_{best['best_alpha']:.2f}_test_metrics.csv\"\n",
    ")\n",
    "print(\"[saved]\", f\"results/best_alpha_old_main_with_CI_{best['best_alpha']:.2f}_test_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result Visualization and Saving\n",
    "import os, json, datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RESULT_DIR = \"results\"\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "ts = dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "k_plot = int(k) if \"k\" in globals() else 10\n",
    "\n",
    "\n",
    "def _check_cols(df, name):\n",
    "    need = {\"alpha\",\"precision_mean\",\"precision_std\",\"health_mean\",\"health_std\"}\n",
    "    miss = [c for c in need if c not in df.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"{name} Missing: {miss}\")\n",
    "\n",
    "_check_cols(df_val,  \"df_val\")\n",
    "_check_cols(df_test, \"df_test\")\n",
    "\n",
    "val_csv  = os.path.join(RESULT_DIR, f\"df_val_{ts}.csv\")\n",
    "test_csv = os.path.join(RESULT_DIR, f\"df_test_{ts}.csv\")\n",
    "df_val.to_csv(val_csv, index=False)\n",
    "df_test.to_csv(test_csv, index=False)\n",
    "\n",
    "best_json = os.path.join(RESULT_DIR, f\"best_alpha_{ts}.json\")\n",
    "with open(best_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"[saved] {val_csv}\")\n",
    "print(f\"[saved] {test_csv}\")\n",
    "print(f\"[saved] {best_json}\")\n",
    "\n",
    "\n",
    "def plot_tradeoff_with_ci(df, split_name: str, k_top: int = 10, save_ts: str = None):\n",
    "    fig, ax1 = plt.subplots(figsize=(8.5, 5))\n",
    "\n",
    "    # Precision@k (左轴)\n",
    "    ax1.fill_between(df[\"alpha\"],\n",
    "                     df[\"precision_mean\"] - 1.96*df[\"precision_std\"],\n",
    "                     df[\"precision_mean\"] + 1.96*df[\"precision_std\"],\n",
    "                     alpha=0.20, label=\"Precision 95% CI\")\n",
    "    p_line, = ax1.plot(df[\"alpha\"], df[\"precision_mean\"], \"-o\", label=f\"Precision@{k_top}\")\n",
    "    ax1.set_xlabel(\"α (health weight)\")\n",
    "    ax1.set_ylabel(f\"Precision@{k_top}\")\n",
    "    ax1.grid(True, alpha=0.25)\n",
    "\n",
    "    # HealthGain@k (右轴)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.fill_between(df[\"alpha\"],\n",
    "                     df[\"health_mean\"] - 1.96*df[\"health_std\"],\n",
    "                     df[\"health_mean\"] + 1.96*df[\"health_std\"],\n",
    "                     alpha=0.20, color=\"tab:orange\", label=\"HealthGain 95% CI\")\n",
    "    h_line, = ax2.plot(df[\"alpha\"], df[\"health_mean\"], \"-s\", color=\"tab:orange\",\n",
    "                       label=f\"HealthGain@{k_top}\")\n",
    "    ax2.set_ylabel(f\"HealthGain@{k_top} (avg HSR diff / 5)\")\n",
    "\n",
    "\n",
    "    ax1.set_title(f\"Health–Accuracy Trade-off (Bootstrap CI) — {split_name} (K={k_top})\")\n",
    "    lines = [p_line, h_line]\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fname = os.path.join(RESULT_DIR, f\"alpha_tradeoff_{split_name.lower()}_{save_ts or ts}.png\")\n",
    "    plt.savefig(fname, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"[saved] {fname}\")\n",
    "    return fname\n",
    "\n",
    "fig_val  = plot_tradeoff_with_ci(df_val,  \"VAL\",  k_plot, ts)\n",
    "fig_test = plot_tradeoff_with_ci(df_test, \"TEST\", k_plot, ts)\n",
    "\n",
    "\n",
    "snap = {\n",
    "    \"best_alpha\":          float(best.get(\"best_alpha\", np.nan)),\n",
    "    \"best_precision_mean\": float(best.get(\"precision\", np.nan)),\n",
    "    \"best_healthgain_mean\":float(best.get(\"healthgain\", np.nan)),\n",
    "    \"k\": k_plot,\n",
    "    \"val_rows\": int(len(df_val)),\n",
    "    \"test_rows\": int(len(df_test)),\n",
    "}\n",
    "snap_csv = os.path.join(RESULT_DIR, f\"snapshot_{ts}.csv\")\n",
    "pd.Series(snap).to_csv(snap_csv)\n",
    "print(f\"[saved] {snap_csv}\")\n",
    "print(\"[snapshot]\", snap)\n",
    "\n",
    "\n",
    "if \"g\" in globals() and \"th_best\" in globals() and \"is_food_score\" in g.columns:\n",
    "    plt.figure(figsize=(8,4.2))\n",
    "    plt.hist(g[\"is_food_score\"], bins=60, alpha=0.8)\n",
    "    plt.axvline(float(th_best), ls=\"--\", label=f\"thr={float(th_best):.3f}\")\n",
    "    plt.title(\"Food probability distribution (Phase 1)\")\n",
    "    plt.xlabel(\"is_food_score\"); plt.ylabel(\"Count\"); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    hist_path = os.path.join(RESULT_DIR, f\"food_prob_hist_{ts}.png\")\n",
    "    plt.savefig(hist_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"[saved] {hist_path}\")\n",
    "else:\n",
    "    print(\"[note] Skip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alpha_rerank_panels(\n",
    "    test_packs, true_pos_map_test,\n",
    "    item_hsr_dict=item_hsr,           \n",
    "    use_df=use,                      \n",
    "    alpha=0.0, k=10,\n",
    "    item_hsr_display_dict=item_hsr_disp \n",
    "))\n",
    "print(alpha_rerank_panels(\n",
    "    test_packs, true_pos_map_test,\n",
    "    item_hsr_dict=item_hsr,\n",
    "    use_df=use,\n",
    "    alpha=1.0, k=10,\n",
    "    item_hsr_display_dict=item_hsr_disp\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b0ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"alpha_search_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "fig_path = os.path.join(output_dir, \"Health_Accuracy_Tradeoff_Test.png\")\n",
    "plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"[saved] Figure saved to: {fig_path}\")\n",
    "\n",
    "val_path = os.path.join(output_dir, \"df_val.csv\")\n",
    "test_path = os.path.join(output_dir, \"df_test.csv\")\n",
    "df_val.to_csv(val_path, index=False)\n",
    "df_test.to_csv(test_path, index=False)\n",
    "print(f\"[saved] Tables saved to: {val_path}, {test_path}\")\n",
    "\n",
    "summary_path = os.path.join(output_dir, \"best_alpha_summary.txt\")\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== Optimal α Summary ===\\n\")\n",
    "    for k, v in best.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "print(f\"[saved] Summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746481b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k = 10 \n",
    "\n",
    "os.makedirs(\"alpha_search_results\", exist_ok=True)\n",
    "out_path = os.path.join(\"alpha_search_results\", \"Health_Accuracy_Tradeoff_Test.png\")\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,5))\n",
    "ax1.set_title(f\"Health–Accuracy Trade-off (Bootstrap CI, K={k}) - TEST\")\n",
    "\n",
    "\n",
    "lower_p = df_test['precision_mean'] - 1.96*df_test['precision_std']\n",
    "upper_p = df_test['precision_mean'] + 1.96*df_test['precision_std']\n",
    "ax1.fill_between(df_test['alpha'], lower_p, upper_p, alpha=0.2, label='Precision 95% CI')\n",
    "p1, = ax1.plot(df_test['alpha'], df_test['precision_mean'], '-o', label=f'Precision@{k}')\n",
    "ax1.set_xlabel('α (health weight)')\n",
    "ax1.set_ylabel(f'Precision@{k}')\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "lower_h = df_test['health_mean'] - 1.96*df_test['health_std']\n",
    "upper_h = df_test['health_mean'] + 1.96*df_test['health_std']\n",
    "ax2.fill_between(df_test['alpha'], lower_h, upper_h, alpha=0.2, label='HealthGain 95% CI')\n",
    "p2, = ax2.plot(df_test['alpha'], df_test['health_mean'], '-s', label=f'HealthGain@{k}')\n",
    "ax2.set_ylabel(f'HealthGain@{k} (avg HSR diff)')\n",
    "\n",
    "\n",
    "lines = [p1, p2]\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='best')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"[saved] {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "axes[0].hist(h[hsr_col].dropna(), bins=20, color='green', alpha=0.6)\n",
    "axes[0].set_title(\"Original HSR distribution\")\n",
    "axes[1].hist(g[\"assigned_hsr_ml\"].dropna(), bins=20, color='orange', alpha=0.6)\n",
    "axes[1].set_title(\"Mapped Grocery HSR distribution\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Original HSR mean:\", h[hsr_col].mean())\n",
    "print(\"Mapped HSR mean:\", g[\"assigned_hsr_ml\"].mean())\n",
    "print(\"SBERT mean similarity:\", g['hsr_top1_sim'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21435962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def candidate_pool_hsr(packs, item_hsr_dict, max_users=2000):\n",
    "    xs = []\n",
    "    for u, cand, *_ in packs[:max_users]:\n",
    "        h = np.array([item_hsr_dict.get(i, np.nan) for i in cand], dtype=float)\n",
    "        xs.extend(h.tolist())\n",
    "    xs = np.array(xs, dtype=float)\n",
    "    xs = xs[np.isfinite(xs)]                 \n",
    "    xs = xs[(xs >= 0.5) & (xs <= 5.0)]       \n",
    "    return xs\n",
    "\n",
    "def simulate_recommend(packs, item_hsr_dict, alpha, k=10, max_users=2000):\n",
    "    xs = []\n",
    "    for u, cand, ytrue, base_scores in packs[:max_users]:\n",
    "        h = np.array([item_hsr_dict.get(i, np.nan) for i in cand], dtype=float)\n",
    "        ok = np.isfinite(h)\n",
    "        if not ok.any(): \n",
    "            continue\n",
    "        h_user = np.where(np.isfinite(h), h, np.nanmean(h[ok]))\n",
    "        z = (1 - alpha) * base_scores + alpha * (h_user / 5.0)\n",
    "        idx = np.argsort(-z)[:k]\n",
    "        xs.extend(h_user[idx].tolist())\n",
    "    xs = np.array(xs, dtype=float)\n",
    "    xs = xs[np.isfinite(xs)]\n",
    "    xs = xs[(xs >= 0.5) & (xs <= 5.0)]\n",
    "    return xs\n",
    "\n",
    "pool = candidate_pool_hsr(test_packs, item_hsr_cont, max_users=2000)\n",
    "rec_a0  = simulate_recommend(test_packs, item_hsr_cont, alpha=0.00, k=10, max_users=2000)\n",
    "rec_a65 = simulate_recommend(test_packs, item_hsr_cont, alpha=0.65, k=10, max_users=2000)\n",
    "\n",
    "m = min(len(pool), len(rec_a0), len(rec_a65), 30000)  \n",
    "pool_s  = rng.choice(pool,   size=m, replace=False)\n",
    "rec0_s  = rng.choice(rec_a0, size=m, replace=True)   \n",
    "rec65_s = rng.choice(rec_a65,size=m, replace=True)\n",
    "\n",
    "bins = np.linspace(0.5, 5.0, 19) \n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.hist(pool_s,  bins=bins, alpha=0.35, density=True, label=\"All candidate items\")\n",
    "plt.hist(rec0_s,  bins=bins, alpha=0.60, density=True, label=\"Baseline top-10 (α=0)\")\n",
    "plt.hist(rec65_s, bins=bins, alpha=0.60, density=True, label=\"α=0.65 top-10\")\n",
    "plt.xlabel(\"HSR score (0.5–5)\"); plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of HSR: All vs Recommended (density, subsampled)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/hsr_distributions_density.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "print(f\"[stats] pool mean={pool.mean():.3f} | α=0 mean={rec_a0.mean():.3f} | α=0.65 mean={rec_a65.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def avg_hsr_stats(alpha, k=10):\n",
    "    top_means = []\n",
    "    cand_means = []\n",
    "    for u, cand, ytrue, base_scores in test_packs:\n",
    "        h = np.array([g.loc[g['item_norm']==i,'assigned_hsr_ml'].mean() for i in cand])\n",
    "        h = np.nan_to_num(h, nan=np.nanmean(h))\n",
    "        h01 = h / 5.0\n",
    "        hybrid = (1 - alpha) * base_scores + alpha * h01\n",
    "        idx = np.argsort(-hybrid)[:k]\n",
    "        top_means.append(np.mean(h[idx]))\n",
    "        cand_means.append(np.mean(h))\n",
    "    return np.mean(cand_means), np.mean(top_means)\n",
    "\n",
    "cand_mean, top_mean = avg_hsr_stats(alpha=0.0)\n",
    "print(f\"Baseline α=0: candidates mean={cand_mean:.3f}, top@10 mean={top_mean:.3f}\")\n",
    "cand_mean, top_mean = avg_hsr_stats(alpha=0.65)\n",
    "print(f\"α=0.65: candidates mean={cand_mean:.3f}, top@10 mean={top_mean:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c2e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def user_health_gain(alpha, k=10):\n",
    "    user_gain = []\n",
    "    for u, cand, ytrue, base_scores in test_packs:\n",
    "        h = np.array([g.loc[g['item_norm']==i,'assigned_hsr_ml'].mean() for i in cand])\n",
    "        h = np.nan_to_num(h, nan=np.nanmean(h))\n",
    "        h01 = h / 5.0\n",
    "        hybrid = (1 - alpha) * base_scores + alpha * h01\n",
    "        idx = np.argsort(-hybrid)[:k]\n",
    "        gain = np.mean(h[idx]) - np.mean(h)\n",
    "        user_gain.append(gain)\n",
    "    return np.array(user_gain)\n",
    "\n",
    "gain_base = user_health_gain(alpha=0.0)\n",
    "gain_alpha = user_health_gain(alpha=0.65)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(gain_base, bins=40, alpha=0.6, label='Baseline α=0')\n",
    "plt.hist(gain_alpha, bins=40, alpha=0.6, label='α=0.65')\n",
    "plt.axvline(0, color='black', linestyle='--')\n",
    "plt.xlabel(\"User-level HealthGain (ΔHSR)\")\n",
    "plt.ylabel(\"Count of users\")\n",
    "plt.title(\"User-level HealthGain distribution\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
